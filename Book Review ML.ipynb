{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Define and Solve an ML Problem of Your Choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab assignment, you will follow the machine learning life cycle and implement a model to solve a machine learning problem of your choosing. You will select a data set and choose a predictive problem that the data set supports.  You will then inspect the data with your problem in mind and begin to formulate a  project plan. You will then implement the machine learning project plan. \n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "1. Build Your DataFrame\n",
    "2. Define Your ML Problem\n",
    "3. Perform exploratory data analysis to understand your data.\n",
    "4. Define Your Project Plan\n",
    "5. Implement Your Project Plan:\n",
    "    * Prepare your data for your model.\n",
    "    * Fit your model to the training data and evaluate your model.\n",
    "    * Improve your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Build Your DataFrame\n",
    "\n",
    "You will have the option to choose one of four data sets that you have worked with in this program:\n",
    "\n",
    "* The \"census\" data set that contains Census information from 1994: `censusData.csv`\n",
    "* Airbnb NYC \"listings\" data set: `airbnbListingsData.csv`\n",
    "* World Happiness Report (WHR) data set: `WHR2018Chapter2OnlineData.csv`\n",
    "* Book Review data set: `bookReviewsData.csv`\n",
    "\n",
    "Note that these are variations of the data sets that you have worked with in this program. For example, some do not include some of the preprocessing necessary for specific models. \n",
    "\n",
    "#### Load a Data Set and Save it as a Pandas DataFrame\n",
    "\n",
    "The code cell below contains filenames (path + filename) for each of the four data sets available to you.\n",
    "\n",
    "<b>Task:</b> In the code cell below, use the same method you have been using to load the data using `pd.read_csv()` and save it to DataFrame `df`. \n",
    "\n",
    "You can load each file as a new DataFrame to inspect the data before choosing your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was perhaps the best of Johannes Steinhof...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This very fascinating book is a story written ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The four tales in this collection are beautifu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The book contained more profanity than I expec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We have now entered a second time of deep conc...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Positive Review\n",
       "0  This was perhaps the best of Johannes Steinhof...             True\n",
       "1  This very fascinating book is a story written ...             True\n",
       "2  The four tales in this collection are beautifu...             True\n",
       "3  The book contained more profanity than I expec...            False\n",
       "4  We have now entered a second time of deep conc...             True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File names of the four data sets\n",
    "adultDataSet_filename = os.path.join(os.getcwd(), \"data\", \"censusData.csv\")\n",
    "airbnbDataSet_filename = os.path.join(os.getcwd(), \"data\", \"airbnbListingsData.csv\")\n",
    "WHRDataSet_filename = os.path.join(os.getcwd(), \"data\", \"WHR2018Chapter2OnlineData.csv\")\n",
    "bookReviewDataSet_filename = os.path.join(os.getcwd(), \"data\", \"bookReviewsData.csv\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(bookReviewDataSet_filename)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Define Your ML Problem\n",
    "\n",
    "Next you will formulate your ML Problem. In the markdown cell below, answer the following questions:\n",
    "\n",
    "1. List the data set you have chosen.\n",
    "2. What will you be predicting? What is the label?\n",
    "3. Is this a supervised or unsupervised learning problem? Is this a clustering, classification or regression problem? Is it a binary classificaiton or multi-class classifiction problem?\n",
    "4. What are your features? (note: this list may change after your explore your data)\n",
    "5. Explain why this is an important problem. In other words, how would a company create value with a model that predicts this label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) I have chosen the Book Review data set\n",
    "2) I will be predicting whether a certain review given about a book is a positive review or not. \n",
    "3) This is a supervised binary classification problem.\n",
    "4) The feature is the review that book readers leave\n",
    "5) A publishing company can make a good use of this model to choose books with good reviews to publish which will svae them from a lot of loss and helps them invest on books with good reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understand Your Data\n",
    "\n",
    "The next step is to perform exploratory data analysis. Inspect and analyze your data set with your machine learning problem in mind. Consider the following as you inspect your data:\n",
    "\n",
    "1. What data preparation techniques would you like to use? These data preparation techniques may include:\n",
    "\n",
    "    * addressing missingness, such as replacing missing values with means\n",
    "    * finding and replacing outliers\n",
    "    * renaming features and labels\n",
    "    * finding and replacing outliers\n",
    "    * performing feature engineering techniques such as one-hot encoding on categorical features\n",
    "    * selecting appropriate features and removing irrelevant features\n",
    "    * performing specific data cleaning and preprocessing techniques for an NLP problem\n",
    "    * addressing class imbalance in your data sample to promote fair AI\n",
    "    \n",
    "\n",
    "2. What machine learning model (or models) you would like to use that is suitable for your predictive problem and data?\n",
    "    * Are there other data preparation techniques that you will need to apply to build a balanced modeling data set for your problem and model? For example, will you need to scale your data?\n",
    " \n",
    " \n",
    "3. How will you evaluate and improve the model's performance?\n",
    "    * Are there specific evaluation metrics and methods that are appropriate for your model?\n",
    "    \n",
    "\n",
    "Think of the different techniques you have used to inspect and analyze your data in this course. These include using Pandas to apply data filters, using the Pandas `describe()` method to get insight into key statistics for each column, using the Pandas `dtypes` property to inspect the data type of each column, and using Matplotlib and Seaborn to detect outliers and visualize relationships between features and labels. If you are working on a classification problem, use techniques you have learned to determine if there is class imbalance.\n",
    "\n",
    "<b>Task</b>: Use the techniques you have learned in this course to inspect and analyze your data. You can import additional packages that you have used in this course that you will need to perform this task.\n",
    "\n",
    "<b>Note</b>: You can add code cells if needed by going to the <b>Insert</b> menu and clicking on <b>Insert Cell Below</b> in the drop-drown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review             0\n",
       "Positive Review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "     |████████████████████████████████| 1.5 MB 15.3 MB/s            \n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2023.8.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)\n",
      "     |████████████████████████████████| 759 kB 49.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.14.1)\n",
      "Requirement already satisfied: click in /home/codio/.local/lib/python3.6/site-packages (from nltk) (7.1.1)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     |████████████████████████████████| 78 kB 1.6 MB/s             \n",
      "\u001b[?25hCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting zipp>=3.1.0\n",
      "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Installing collected packages: zipp, importlib-resources, tqdm, regex, nltk\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/codio/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script nltk is installed in '/home/codio/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed importlib-resources-5.4.0 nltk-3.6.7 regex-2023.8.8 tqdm-4.64.1 zipp-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/codio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/codio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0  This was perhaps the best of Johannes Steinhof...   \n",
      "1  This very fascinating book is a story written ...   \n",
      "2  The four tales in this collection are beautifu...   \n",
      "3  The book contained more profanity than I expec...   \n",
      "4  We have now entered a second time of deep conc...   \n",
      "\n",
      "                                    Processed_Review  \n",
      "0  perhaps best johannes steinhoff books since de...  \n",
      "1  fascinating book story written form numerous l...  \n",
      "2  four tales collection beautifully composed art...  \n",
      "3  book contained profanity expected read book ri...  \n",
      "4  entered second time deep concern science math ...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize: Split the text into words (tokens)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lowercase: Convert all words to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    # Remove stopwords: Remove common words that don’t add much meaning\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    words = [word for word in words if re.match(r'[a-zA-Z]', word)]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to the 'Review' column\n",
    "df['Processed_Review'] = df['Review'].apply(preprocess_text)\n",
    "\n",
    "# Show the first few rows of the dataframe to inspect the processed reviews\n",
    "print(df[['Review', 'Processed_Review']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Define Your Project Plan\n",
    "\n",
    "Now that you understand your data, in the markdown cell below, define your plan to implement the remaining phases of the machine learning life cycle (data preparation, modeling, evaluation) to solve your ML problem. Answer the following questions:\n",
    "\n",
    "* Do you have a new feature list? If so, what are the features that you chose to keep and remove after inspecting the data? \n",
    "* Explain different data preparation techniques that you will use to prepare your data for modeling.\n",
    "* What is your model (or models)?\n",
    "* Describe your plan to train your model, analyze its performance and then improve the model. That is, describe your model building, validation and selection plan to produce a model that generalizes well to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thre's only have one feature which is the Review column, which I obviously intend to keep. The data preparation techniques I used include text cleaning by converting the text to lowercase, removing non-alphabetic characters, tokenizing the text, and removing stopwords. I then apply TF-IDF vectorization to transform the text data into numerical feature vectors.The model I will use is the Logistic Regression Model. I will fit the logistic regression model to the training data, make predictions and use evaluation metrics like the AUC and ROC scores to evaluate the performace of the model. Finally, I will experiment with different values of hyperparameters such as min_def and max_def to find the optimum performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Implement Your Project Plan\n",
    "\n",
    "<b>Task:</b> In the code cell below, import additional packages that you have used in this course that you will need to implement your project plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Use the rest of this notebook to carry out your project plan. \n",
    "\n",
    "You will:\n",
    "\n",
    "1. Prepare your data for your model.\n",
    "2. Fit your model to the training data and evaluate your model.\n",
    "3. Improve your model's performance by performing model selection and/or feature selection techniques to find best model for your problem.\n",
    "\n",
    "Add code cells below and populate the notebook with commentary, code, analyses, results, and figures as you see fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1973,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['Positive Review'] \n",
    "X = df['Review']\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500     There is a reason this book has sold over 180,...\n",
       "1047    There is one thing that every cookbook author ...\n",
       "1667    Being an engineer in the aerospace industry I ...\n",
       "1646    I have no idea how this book has received the ...\n",
       "284     It is almost like dream comes true when I saw ...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.75, random_state=1234)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min Document Frequency Value: 1\n",
      "AUC on the test data: 0.9268\n",
      "The size of the feature space: 138486\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('there', 119835), ('is', 61671), ('reason', 97323), ('this', 120815), ('book', 18054)]:\n",
      "Glimpse of first 5 stop words \n",
      "[]:\n",
      "\n",
      "Min Document Frequency Value: 3\n",
      "AUC on the test data: 0.9280\n",
      "The size of the feature space: 17684\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('there', 15005), ('is', 7641), ('reason', 11976), ('this', 15162), ('book', 2272)]:\n",
      "Glimpse of first 5 stop words \n",
      "['most others', 'israel will', 'swath of', 'naive catherine', 'they used']:\n",
      "\n",
      "Min Document Frequency Value: 6\n",
      "AUC on the test data: 0.9258\n",
      "The size of the feature space: 7337\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('there', 6166), ('is', 3114), ('reason', 4913), ('this', 6239), ('book', 879)]:\n",
      "Glimpse of first 5 stop words \n",
      "['most others', 'israel will', 'swath of', 'naive catherine', 'they used']:\n",
      "\n",
      "Min Document Frequency Value: 8\n",
      "AUC on the test data: 0.9247\n",
      "The size of the feature space: 5231\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('there', 4381), ('is', 2220), ('reason', 3525), ('this', 4434), ('book', 633)]:\n",
      "Glimpse of first 5 stop words \n",
      "['most others', 'israel will', 'swath of', 'naive catherine', 'they used']:\n",
      "\n",
      "Min Document Frequency Value: 10\n",
      "AUC on the test data: 0.9195\n",
      "The size of the feature space: 4023\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('there', 3352), ('is', 1687), ('reason', 2699), ('this', 3396), ('book', 464)]:\n",
      "Glimpse of first 5 stop words \n",
      "['most others', 'israel will', 'swath of', 'naive catherine', 'they used']:\n",
      "\n",
      "Min Document Frequency Value: 100\n",
      "AUC on the test data: 0.8463\n",
      "The size of the feature space: 257\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('there', 199), ('is', 102), ('this', 207), ('book', 35), ('has', 81)]:\n",
      "Glimpse of first 5 stop words \n",
      "['most others', 'israel will', 'swath of', 'naive catherine', 'they used']:\n",
      "\n",
      "Min Document Frequency Value: 1000\n",
      "AUC on the test data: 0.6435\n",
      "The size of the feature space: 9\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 3), ('this', 7), ('book', 1), ('it', 4), ('to', 8)]:\n",
      "Glimpse of first 5 stop words \n",
      "['most others', 'israel will', 'swath of', 'naive catherine', 'they used']:\n"
     ]
    }
   ],
   "source": [
    "for min_df in [1, 3, 6, 8, 10,100,1000]:\n",
    "    \n",
    "    print('\\nMin Document Frequency Value: {0}'.format(min_df))\n",
    "    \n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=min_df, ngram_range=(1,2))\n",
    "\n",
    "    \n",
    "    tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "    \n",
    "    X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "   \n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    \n",
    "    probability_predictions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "    \n",
    "    auc = roc_auc_score(y_test, probability_predictions)\n",
    "    print('AUC on the test data: {:.4f}'.format(auc))\n",
    "\n",
    "   \n",
    "    len_feature_space = len(tfidf_vectorizer.vocabulary_)\n",
    "    print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "    \n",
    "    \n",
    "    first_five = list(tfidf_vectorizer.vocabulary_.items())[0:5]\n",
    "    print('Glimpse of first 5 entries of the mapping of a word to its column/feature index \\n{}:'.format(first_five))\n",
    "\n",
    "    \n",
    "    first_five_stop = list(tfidf_vectorizer.stop_words_)[0:5]\n",
    "    print('Glimpse of first 5 stop words \\n{}:'.format(first_five_stop))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review #1:\n",
      "\n",
      "This is the best review of the JFK assassination that I have seen. There is still a large \"assassination industry,\" which can afford to find documents that you haven't read and charge you with ignorance if you haven't read them, and find 15 more if you read them. This gives a common-sense overview that seems quite reasonable. I trust it. I am always willing to consider other opinions, but the balance of evidence has always indicated that Oswald acted alone.\n",
      "\n",
      "It would be nice to have a new edition of this book..\n",
      "\n",
      "\n",
      "Prediction: Is this a good review? True\n",
      "\n",
      "Actual: Is this a good review? True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Review #1:\\n')\n",
    "print(X_test.to_numpy()[13])\n",
    "\n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(class_label_predictions[13])) \n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review #2:\n",
      "\n",
      "A wonderful wonderful book.Although the book was written some 150 year ago, Mary Eliza Rogers takes you into the intimacies of daily life in Palestine in the 1850's as if it was occurring today. She writes from her  heart with honesty,integrity and a clear mind. And although written at a  time of Victorian prejudicies and colonialism she writes without bias or  judgement. From her beautiful and colourful descriptions one can envisage  the Holy Land as it was before undergoing the process of modernisation and  change. For anyone who has any attachment to this land it is a truly  wonderful and personal experience to read this book\n",
      "\n",
      "\n",
      "Prediction: Is this a good review? True\n",
      "\n",
      "Actual: Is this a good review? True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Review #2:\\n')\n",
    "print(X_test.to_numpy()[250])\n",
    "\n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(class_label_predictions[250])) \n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[250]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
